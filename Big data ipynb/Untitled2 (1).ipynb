{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3b8344d-83f6-4345-a1ac-0df1d2cf1047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/27 13:15:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized with custom configurations\n",
      "Data loaded into Pandas DataFrame\n",
      "Pandas DataFrame converted to Spark DataFrame\n",
      "Missing values handled\n",
      "Date column converted\n",
      "Data partitioned by date\n",
      "Output path: /home/b1519e89-0a35-4c07-9c0a-b03ec2293045/istanbul_stock_exchange\n",
      "Data saved to Parquet format\n",
      "+----------+------------+-------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|      date|TL BASED ISE|USD BASED ISE|          SP|         DAX|        FTSE|      NIKKEI|     BOVESPA|          EU|          EM|\n",
      "+----------+------------+-------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|      NULL|         ISE|          ISE|          SP|         DAX|        FTSE|      NIKKEI|     BOVESPA|          EU|          EM|\n",
      "|2009-01-05| 0.035753708|  0.038376187|-0.004679315| 0.002193419| 0.003894376|           0| 0.031190229| 0.012698039| 0.028524462|\n",
      "|2009-01-06| 0.025425873|  0.031812743| 0.007786738| 0.008455341| 0.012865611| 0.004162452|  0.01891958| 0.011340652| 0.008772644|\n",
      "|2009-01-07| -0.02886173| -0.026352966|-0.030469134|-0.017833062|-0.028734593| 0.017292932|-0.035898576|-0.017072795|-0.020015412|\n",
      "|2009-01-08|-0.062208079| -0.084715902| 0.003391364|-0.011726277|-0.000465999|-0.040061309| 0.028283152|-0.005560959|-0.019423778|\n",
      "+----------+------------+-------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Data read back from Parquet files and displayed\n",
      "Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Initialize Spark session with custom configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataIngestion\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized with custom configurations\")\n",
    "\n",
    "# Load the cleaned dataset into a Pandas DataFrame\n",
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "print(\"Data loaded into Pandas DataFrame\")\n",
    "\n",
    "# Convert the Pandas Data Frame to a Spark Data Frame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "print(\"Pandas DataFrame converted to Spark DataFrame\")\n",
    "\n",
    "# Transform: Handle missing values (example: fill with 0 or mean)\n",
    "spark_df = spark_df.fillna(0)\n",
    "print(\"Missing values handled\")\n",
    "\n",
    "# Convert the date column to date type if applicable\n",
    "spark_df = spark_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "print(\"Date column converted\")\n",
    "\n",
    "# Partition the data by year for better performance if applicable\n",
    "spark_df = spark_df.repartitionByRange(\"date\")\n",
    "print(\"Data partitioned by date\")\n",
    "\n",
    "# Define output path and ensure the directory exists\n",
    "output_path = os.path.expanduser(\"~/istanbul_stock_exchange\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Output path: {output_path}\")\n",
    "\n",
    "# Save the DataFrame to the specified path in Parquet format\n",
    "spark_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(\"Data saved to Parquet format\")\n",
    "\n",
    "# Read back the saved Parquet files to verify\n",
    "spark_df_read = spark.read.parquet(output_path)\n",
    "spark_df_read.show(5)\n",
    "print(\"Data read back from Parquet files and displayed\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86374da0-5543-46ef-adac-105f33580d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|      date|TL BASED ISE|USD BASED ISE|          SP|         DAX|        FTSE|      NIKKEI|     BOVESPA|          EU|          EM|\n",
      "+----------+------------+-------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|      NULL|         ISE|          ISE|          SP|         DAX|        FTSE|      NIKKEI|     BOVESPA|          EU|          EM|\n",
      "|2009-01-05| 0.035753708|  0.038376187|-0.004679315| 0.002193419| 0.003894376|           0| 0.031190229| 0.012698039| 0.028524462|\n",
      "|2009-01-06| 0.025425873|  0.031812743| 0.007786738| 0.008455341| 0.012865611| 0.004162452|  0.01891958| 0.011340652| 0.008772644|\n",
      "|2009-01-07| -0.02886173| -0.026352966|-0.030469134|-0.017833062|-0.028734593| 0.017292932|-0.035898576|-0.017072795|-0.020015412|\n",
      "|2009-01-08|-0.062208079| -0.084715902| 0.003391364|-0.011726277|-0.000465999|-0.040061309| 0.028283152|-0.005560959|-0.019423778|\n",
      "|2009-01-09| 0.009859905|  0.009658112|-0.021533208|-0.019872754|-0.012709717|-0.004473502| -0.00976388|-0.010988634|-0.007802212|\n",
      "|2009-01-12|-0.029191028| -0.042361155|-0.022822626|-0.013525735|-0.005025533|-0.049038532|-0.053849474|-0.012451259|-0.022629745|\n",
      "|2009-01-13| 0.015445348| -0.000272183| 0.001756552|-0.017673622|-0.006141454|           0|  0.00357202|-0.012220196|-0.004827138|\n",
      "|2009-01-14|-0.041167612|  -0.03555248|-0.034032484|-0.047383437|-0.050945198| 0.002912354|-0.040301613|-0.045220199|-0.008676646|\n",
      "|2009-01-15| 0.000661905| -0.017267844| 0.001328305|-0.019550621|-0.014334659|-0.050447588| 0.030313602|-0.012070424|-0.023428729|\n",
      "|2009-01-16| 0.022037345|  0.032278032| 0.007533126|  0.00679078| 0.006289177| 0.025453186| 0.004866686| 0.008560858| 0.010916893|\n",
      "|2009-01-19|-0.022692465| -0.044348781|-0.054261984|-0.011549904|-0.009351296| 0.003238918|-0.013151015|-0.012045182|-0.004029004|\n",
      "|2009-01-20|-0.013708704| -0.029661366|           0|-0.017833593|-0.004170789|-0.023411498|-0.040899261|-0.015088013|-0.024107193|\n",
      "|2009-01-21| 0.000864697|   0.00152943| 0.042572033| 0.005011186|-0.007728867|-0.020561328| 0.033532127|-0.003338563|-0.005092242|\n",
      "|2009-01-22| -0.00381506|  0.005043164|-0.015278458|-0.009841399|-0.001898399| 0.018817884|-0.016981713| -0.00655222|-0.003227249|\n",
      "|2009-01-23|  0.00566126| -0.010007954| 0.005363236|-0.009640052| 7.40311e-05|-0.038808465| 0.006261036|-0.003620056|-0.008077319|\n",
      "|2009-01-26| 0.046831302|  0.061708176| 0.005537856| 0.034786791| 0.037891115|-0.008181598| 0.009838156| 0.032799581| 0.010319685|\n",
      "|2009-01-27|-0.006634978|   0.01094866| 0.010866312|-0.000797661|-0.003474788| 0.048148148|  0.00492178|-0.002641806| 0.006344498|\n",
      "|2009-01-28| 0.034566982|   0.03587086| 0.033006834| 0.044182012| 0.023747818| 0.005594001| 0.038724652|   0.0299742| 0.022104103|\n",
      "|2009-01-29|-0.020528213| -0.020271848|-0.033681051|-0.020255911| -0.02477352| 0.017723189|-0.014750158|-0.023108586| 0.000408697|\n",
      "+----------+------------+-------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ParquetFile\").getOrCreate()\n",
    "\n",
    "# Update the file path to the correct location\n",
    "directory_path = \"/home/b1519e89-0a35-4c07-9c0a-b03ec2293045/istanbul_stock_exchange\"\n",
    "file_path = os.path.join(directory_path, \"part-00000-ac3332bf-f7a6-434b-9ca2-90ae5f136c88-c000.snappy.parquet\")\n",
    "\n",
    "# Load Parquet file\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# Show the dataframe\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5891ef4-b6bd-4082-a46d-bba41eef3b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
